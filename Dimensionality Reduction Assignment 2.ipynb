{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2e4bf8-947f-4249-a1b5-1c7842830a52",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583f727-b722-41aa-98c6-d07009743906",
   "metadata": {},
   "source": [
    "### Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "attachments": {
    "cdc70966-3580-431f-be86-b8db3babcfec.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "1753f358-ab11-4739-8539-dc363a5c7bbe",
   "metadata": {},
   "source": [
    "![images.png](attachment:cdc70966-3580-431f-be86-b8db3babcfec.png)\n",
    "\n",
    "A projection in the context of Principal Component Analysis (PCA) is a mathematical transformation used to map high-dimensional data onto a lower-dimensional subspace while preserving as much of the data's variance as possible. PCA leverages projections to reduce the dimensionality of data, making it more manageable and easier to analyze while retaining essential information.\n",
    "\n",
    "*How are projections used in PCA?*\n",
    "\n",
    "1. **Centering the Data:** PCA starts by centering the data, which means subtracting the mean (average) of each feature from the data. This ensures that the data is centered around the origin.\n",
    "\n",
    "2. **Covariance Matrix:** PCA calculates the covariance matrix of the centered data. The covariance matrix describes how features relate to each other and helps PCA identify which dimensions contain the most variance and information.\n",
    "\n",
    "3. **Eigenvalue and Eigenvector Calculation:** PCA computes the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the directions (principal components) in which the data varies the most, and eigenvalues quantify the variance along these directions.\n",
    "\n",
    "4. **Projection:** The eigenvectors corresponding to the largest eigenvalues represent the principal components. To reduce the dimensionality of the data, you select a subset of these eigenvectors (usually in descending order of eigenvalues) and project the data onto the space defined by these eigenvectors. This projection transforms the high-dimensional data into a lower-dimensional space.\n",
    "\n",
    "The projected data retains as much of the original data's variance as possible, and the first few principal components capture the most important patterns. By selecting a subset of these principal components, you can reduce the dimensionality of your data while retaining the most critical information, making it easier to analyze and work with.\n",
    "\n",
    "In summary, PCA uses projections to transform high-dimensional data into a lower-dimensional space, simplifying the data while preserving its essential variance and patterns. It's a valuable technique for dimensionality reduction and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56674555-d331-4fe0-8c13-067716e735a2",
   "metadata": {},
   "source": [
    "### Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab9ddb-c22e-49a3-a1ac-0bc85aacca4d",
   "metadata": {},
   "source": [
    "The optimization problem in PCA tries to find a way to reduce the dimensions of data while keeping as much important information as possible. It does this by looking for new directions (eigenvectors) in the data where the variance is the highest. These directions become the new, smaller dimensions. The goal is to pick these directions in a way that minimizes the difference between the original data and the data when it's projected onto these new dimensions. In simple terms, PCA tries to make data smaller but still useful for analysis by finding the best directions along which the data varies the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d594b5-7ccb-4417-b2bc-a75d81de2613",
   "metadata": {},
   "source": [
    "### Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbe93e-24b1-4743-8ca8-a842116b4e4b",
   "metadata": {},
   "source": [
    "Covariance matrices and PCA are closely linked:\n",
    "\n",
    "- **Covariance Matrix:** It shows how different features in your data change together. Positive values mean they tend to increase together, and negative values mean they move in opposite directions.\n",
    "\n",
    "- **PCA:** It uses the covariance matrix to find the most important directions in your data, called principal components. These directions capture the most variation in your data.\n",
    "\n",
    "- **Dimension Reduction:** PCA selects some of these principal components to reduce the data's dimensionality while keeping the most critical information. It simplifies your data while retaining the essential patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6ce8a-d48b-49af-acf5-93821d4d4565",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8246e5e-482f-414f-9636-8b968740a407",
   "metadata": {},
   "source": [
    "\n",
    "| **Number of Principal Components** | **Impact on PCA Performance**                                   |\n",
    "|----------------------------------|--------------------------------------------------------------|\n",
    "| More Components                   | - Higher variance retention. - Better data representation. - More complexity. - Increased risk of overfitting. - Increased computational cost. |\n",
    "| Fewer Components                  | - Greater dimensionality reduction. - Simpler data representation. - Potential loss of information. - Reduced risk of overfitting. - Improved computational efficiency. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1213df3-c9be-479d-bcd2-b28045f589ff",
   "metadata": {},
   "source": [
    "### Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0583de0-4252-4466-836f-79268513951e",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a dimensionality reduction technique that identifies important features by examining how much variance each feature contributes to the data. In other words, it focuses on the features that explain the most variation within the dataset. By selecting these high-variance features, PCA effectively performs feature selection, helping to simplify the data while retaining its most influential characteristics. This process is especially useful in reducing the dimensionality of datasets with many features, making them more manageable for analysis or modeling.\n",
    "\n",
    "\n",
    "- **Benefits of PCA for Feature Selection:**\n",
    "  - Simplifies data through dimensionality reduction.\n",
    "  - Reduces noise by emphasizing high-variance features.\n",
    "  - Improves model efficiency with a smaller feature set.\n",
    "  - Enhances interpretability of selected components.\n",
    "  - Mitigates overfitting by retaining relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65b62c-ff00-4f29-bb57-cf1bfb525e2b",
   "metadata": {},
   "source": [
    "### Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231427c3-6cd1-479c-8e12-75625ef1904f",
   "metadata": {},
   "source": [
    "Common applications of PCA in data science and machine learning include:\n",
    "\n",
    "- **Dimensionality Reduction:** Making complex data simpler to analyze.\n",
    "- **Image Compression:** Reducing image size while keeping important details.\n",
    "- **Face Recognition:** Identifying faces accurately.\n",
    "- **Anomaly Detection:** Detecting unusual patterns or outliers in data.\n",
    "- **Eigenfaces:** Recognizing faces efficiently.\n",
    "- **Natural Language Processing:** Simplifying text analysis.\n",
    "- **Spectral Data Analysis:** Analyzing spectral data.\n",
    "- **Market Basket Analysis:** Understanding shopping behavior.\n",
    "- **Bioinformatics:** Identifying genetic patterns.\n",
    "- **Climate and Environmental Science:** Analyzing environmental data.\n",
    "- **Quality Control and Manufacturing:** Monitoring manufacturing processes.\n",
    "- **Neuroscience:** Simplifying brain activity data.\n",
    "\n",
    "PCA helps simplify complex data for various purposes, making analysis and modeling more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f306a0-e283-4303-aca9-e21c1604fb25",
   "metadata": {},
   "source": [
    "### Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f548d05-55d8-4c85-b0bd-3a0f3161c324",
   "metadata": {},
   "source": [
    "In the context of Principal Component Analysis (PCA), \"spread\" and \"variance\" are closely related concepts:\n",
    "\n",
    "- **Spread:** Spread refers to the extent or distribution of data points in a dataset. It describes how data is dispersed or how much it covers the available space. In PCA, we often think about the spread of data along the principal components, which are the directions along which the data varies the most.\n",
    "\n",
    "- **Variance:** Variance is a statistical measure that quantifies how data points deviate from the mean or average. It indicates the spread or dispersion of data points in a single dimension or along a particular axis.\n",
    "\n",
    "The relationship between spread and variance in PCA is as follows:\n",
    "\n",
    "- PCA identifies principal components (eigenvectors) that capture the directions in which the data exhibits the most variance. The first principal component represents the direction of maximum variance, and subsequent components capture decreasing variances, forming an ordered hierarchy.\n",
    "\n",
    "- When we talk about \"spread\" in PCA, we often refer to how data points are distributed along these principal components. The spread along each principal component corresponds to the variance explained by that component.\n",
    "\n",
    "- The total spread of the data can be thought of as the sum of the variances along all the principal components. In other words, the total variance of the data is the sum of the variances explained by each principal component.\n",
    "\n",
    "In summary, in PCA, spread and variance are related in the sense that spread along a principal component corresponds to the variance explained by that component. The cumulative variance across all principal components accounts for the total spread of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3a093-8c7a-4309-825f-b5977dc73ae0",
   "metadata": {},
   "source": [
    "### Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff5b61-2a4f-4196-b635-2390eca07805",
   "metadata": {},
   "source": [
    "PCA identifies principal components using spread and variance in this way:\n",
    "\n",
    "1. It calculates how data spreads using the covariance matrix.\n",
    "\n",
    "2. It finds the directions in which data spreads the most, called principal components, by looking at the eigenvalues. The one with the highest eigenvalue points in the direction of maximum spread.\n",
    "\n",
    "3. These principal components become the new basis for data. The first one captures the most spread, and so on.\n",
    "\n",
    "4. Data is projected onto these components to simplify it while keeping essential patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6a9c0-8873-4420-a6cf-c0e685b19d93",
   "metadata": {},
   "source": [
    "### Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f128400-5ae5-4101-8231-c18e26a93845",
   "metadata": {},
   "source": [
    "PCA handles data with high variance in some dimensions and low variance in others by giving more attention to the dimensions with high variance. It emphasizes the dimensions that matter most, effectively reducing the influence of dimensions with low variance. This simplifies the data and retains important patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893eaa0b-62e9-4b86-8aee-c85aef534b0a",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
